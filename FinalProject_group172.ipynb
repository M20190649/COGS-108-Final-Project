{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyzed 11 quarters worth of Uber movement data from 2016-2018, where each quarter corresponds to a season (Winter, Spring, Summer, Fall). We did this by identifying shared routes across these 11 separate datasets and then combining information from different years for each quarter. We were then able to compare one quarter’s travel times to another. Finally, we performed a t-test to determine if the mean travel times between quarters were statistically different from one another and other data analysis and visualization techniques to identify any potential patterns between travel time and season.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Brendan Wong\n",
    "- Pooja Yadav\n",
    "- Kaila Lee\n",
    "- Rajandeep Kaur\n",
    "- Zoey Chesny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members IDs\n",
    "\n",
    "- A15749312\n",
    "- A13997099\n",
    "- A12792644\n",
    "- A13736425\n",
    "- A13303136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do various quarters/seasons of the year affect Uber travel times during peak commute hours in San Francisco?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emissions from vehicles have been adding to the everlasting modern-day problems of air pollution and traffic. The introduction of electric vehicles along with a greater emphasis placed on walking, biking, and/or using micro-mobility resources for transportation has reduced the impact to the growing gas emission problem as well as traffic congestion. However, there is still a reliance on vehicles to get people to their destinations, which is what rideshare services alleviate. These programs allow for multiple people to join a ride going in a similar destination, serving as a carpool, which saves gas, time, and money. \n",
    "\n",
    "Many people have contemplated either buying a car or just continuing to use rideshare options, especially when Uber and Lyft constantly promote their services with discount codes. New vehicles cost tens of thousand dollars, and research shows that cars are not even used 95% of the time(Barter). On average, a car is in usage for 6 out of the 168 hours of the week. This number is so small and on top of that the cost of owning a car continues to increase because of the maintenance, insurance, and the possibility of crashes. \n",
    "\n",
    "Rideshare programs such as Uber and Lyft are almost ubiquitous in our modern day world. Studies have shown that rideshare services have increased 37% from 1.9 billion to 2.61 billion people from 2016 to 2017. Both Uber and Lyft claim that one of their driving principles revolves around reducing traffic congestion through minimizing car ownership and usage. In urban cities such as San Francisco, Los Angeles and New York, analyzing peak commute times would prove to be a sound indicator of whether these companies are alleviating the flow of traffic. Currently, there are few projects that examine the direct relationship between rideshare usage and traffic during commute times. \n",
    "\n",
    "According to the automobility report, studies have shown that Lyft and Uber are actually creating more traffic and congestion instead of reducing it. For example, the report noted that Uber and Lyft added 5.7 billion miles of driving the most populated cities, contributing to around a 160% increase in driving in urban cities.\n",
    "\n",
    "To investigate, we would analyze accumulated data about traffic patterns during peak commute times in densely populated cities since the onset of rideshare popularity, in particular, San Francisco. We would also look at specific usage trends with regards to rideshare program data to draw correlations between mean commute time and season.\n",
    "\n",
    "\n",
    "References (include links):\n",
    "- 1) https://www.ucsusa.org/clean-vehicles/electric-vehicles/CA-air-quality-equity\n",
    "- 2) https://www.reinventingparking.org/2013/02/cars-are-parked-95-of-time-lets-check.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that the different seasons will impact Uber travel times. Specifically, we predict that driving travel times will increase during the winter months (December - February) due to the cold weather conditions. In contrast, we predict that Uber travel times during peak commute times will decrease during the summer and autumn months (June - November) as a result of warmer weather. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uber Movement Data\n",
    "\n",
    "Most of our data was pulled from Uber's Movement Dataset. Each row is the aggregated mean and standard deviation of travel time and geometric travel time over the course of each quarter in the fiscal year. At the start of the project, quarter 1 of 2016 through quarter 3 of 2018 were available. \n",
    "\n",
    "### We used 11 different Uber Movement Datasets \n",
    "#### (one per quarter for q1-2016 to q3-2018)\n",
    "\n",
    "- Dataset Name: Uber Movement Data\n",
    "- Link to the dataset: https://movement.uber.com/explore/san_francisco/travel-times/query?lang=en-US&si=1277&ti=&ag=censustracts&dt[tpb]=ALL_DAY&dt[wd;]=1,2,3,4,5,6,7&dt[dr][sd]=2018-12-01&dt[dr][ed]=2018-12-31&cd=&sa;=&sdn=&lat.=37.7749295&lng.=-122.4547777&z.=12\n",
    "- Number of observations:\n",
    "\n",
    "|Year|Quarter|Number of Observations|\n",
    "|---|---|---|\n",
    "|2016|1|5684666|\n",
    "|2016|2|6598363|\n",
    "|2016|3|7428235|\n",
    "|2016|4|7735670|\n",
    "|2017|1|7590838|\n",
    "|2017|2|7983524|\n",
    "|2017|3|8410747|\n",
    "|2017|4|8789139|\n",
    "|2018|1|8941177|\n",
    "|2018|2|9226295|\n",
    "|2018|3|9613339|\n",
    "\n",
    "\n",
    "- Features/variables Present: sourceid (source id), dstid (destination id), hod (hour of day), mean_travel_time (mean travel time), standard_deviation_travel_time (standard deviation travel time), geometric_mean_travel_time, geometric_standard_deviation_travel_time\n",
    "- Features/variables we will use: sourceid (source id), dstid (destination id), hod (hour of day), mean_travel_time (mean travel time)\n",
    "\n",
    "\n",
    "Uber Movement was an initiative spearheaded by Uber to ensure that their data is accessible and useful for cities to inform the future of urban mobility. This dataset, provided by Uber, describes the amount of traffic and movement between various cities around the world. We have access to data about Uber usage in our target cities including San Francisco, Los Angeles and New York. The variables that we can manipulate include the origin and destination locations for the trip in addition to the date-time range, which correspond directly to the average travel time, displayed in minutes. This tool allows us to filter by specific weekdays or weekends and time blocks during the day, which allows us to specifically look at Uber usage during peak commute times. The number of observations varies based on the origin and destination locations. \n",
    "\n",
    "For example, we selected a highly congested route in San Francisco during the weekdays during peak morning commute time from 7am-10am. We were able to download a CSV file to observe data about this specific route’s traffic details, which resulted in 1,164 trip results. The average travel time for this route was 41 min and 50 sec in comparison to the same route during non-peak hours including the afternoon and weekends, where the average travel time was 30 min and 51 sec. \n",
    "\n",
    "We can also view the mean and range of travel times that correspond to our designated route over the span of a week or other various periods of time.  The dataset provides bar charts that we can use to visualize the number of Uber drivers and compare it to our other datasets such as DataSF. Additionally, this dataset aligns with our ethical considerations as all data is completely anonymized and aggregated to maintain driver and ride privacy. \n",
    "\n",
    "## How we combine 11 datasets \n",
    "\n",
    "We will peform our analysis only on routes that are present in all 11 datasets so that we can cleanly compare across quarters, ensuring the route is present for each comparison. We will also limit our analysis data to 100 routes because working with 11 datasets that each have millions of entries will be very computationall expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The cell below includes the packages used for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "#Display plots directly in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "import patsy\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our data was pulled from [Uber's Movement Dataset](https://movement.uber.com/?lang=en-US). Each row is the aggregated mean and standard deviation of travel time and geometric travel time over the course of each quarter in the fiscal year. At the start of the project, quarter 1 of 2016 through quarter 3 of 2018 were available. Sources and destinations are determined by 'sourceid' and 'dstid'. \n",
    "\n",
    "- 1) *Oberve the data:* check for missing values, look at shape of data, look at variables and observations\n",
    "- 2) *Filter all 11 datasets:* We decided to choose the inner join of sources and destinations with over 10,000 rows as a source, and 5,000 as a destination. Furthermore we'd be tracking commute times so we'd be further filtering from rush hour in the Bay Area, all the time in San Francisco, but typically 7am - 10am and 3pm - 7pm.\n",
    "- 3) *Observe filtered data:* Look at filtered data and decide how to clean it up. This involves reading in a filtered csv file to a dataframe and dropping unnecessary columns\n",
    "- 4) *Find route method:* Create a method to find each route in a filtered dataframe: There are still multiple row entries for one route (a particular source and destination). This is because there are multiple hours of days corresponding to the one route. Here I take the average across these hours of the day and put them in one entry for that route. \n",
    "- 5) *Test find_route method*: I test the above method on the single df from step (3) to make sure the output is as expected\n",
    "- 6) *Run find_route method on all filtered datasets*: save the new dataframes to a csv file so this code does not have to be run multiple times as it is very time consuming. \n",
    "- 7) *Create route_matching method*: This method find 100 routes that have the same source and destination across all 11 datasets and creates a new dataframe with route_name and corresponding mean travel time for each of the 4 quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "sourceid                                    0\n",
      "dstid                                       0\n",
      "hod                                         0\n",
      "mean_travel_time                            0\n",
      "standard_deviation_travel_time              0\n",
      "geometric_mean_travel_time                  0\n",
      "geometric_standard_deviation_travel_time    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sourceid</th>\n",
       "      <th>dstid</th>\n",
       "      <th>hod</th>\n",
       "      <th>mean_travel_time</th>\n",
       "      <th>standard_deviation_travel_time</th>\n",
       "      <th>geometric_mean_travel_time</th>\n",
       "      <th>geometric_standard_deviation_travel_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4806669</th>\n",
       "      <td>1498</td>\n",
       "      <td>1069</td>\n",
       "      <td>0</td>\n",
       "      <td>387.64</td>\n",
       "      <td>165.81</td>\n",
       "      <td>363.88</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732148</th>\n",
       "      <td>1742</td>\n",
       "      <td>1743</td>\n",
       "      <td>0</td>\n",
       "      <td>594.00</td>\n",
       "      <td>216.39</td>\n",
       "      <td>532.56</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732147</th>\n",
       "      <td>1743</td>\n",
       "      <td>1733</td>\n",
       "      <td>0</td>\n",
       "      <td>1319.04</td>\n",
       "      <td>326.51</td>\n",
       "      <td>1284.17</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732146</th>\n",
       "      <td>1772</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>760.37</td>\n",
       "      <td>251.59</td>\n",
       "      <td>723.62</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281306</th>\n",
       "      <td>629</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>1102.59</td>\n",
       "      <td>329.58</td>\n",
       "      <td>1061.21</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sourceid  dstid  hod  mean_travel_time  \\\n",
       "4806669      1498   1069    0            387.64   \n",
       "2732148      1742   1743    0            594.00   \n",
       "2732147      1743   1733    0           1319.04   \n",
       "2732146      1772   1443    0            760.37   \n",
       "5281306       629     96    0           1102.59   \n",
       "\n",
       "         standard_deviation_travel_time  geometric_mean_travel_time  \\\n",
       "4806669                          165.81                      363.88   \n",
       "2732148                          216.39                      532.56   \n",
       "2732147                          326.51                     1284.17   \n",
       "2732146                          251.59                      723.62   \n",
       "5281306                          329.58                     1061.21   \n",
       "\n",
       "         geometric_standard_deviation_travel_time  \n",
       "4806669                                      1.39  \n",
       "2732148                                      1.83  \n",
       "2732147                                      1.25  \n",
       "2732146                                      1.36  \n",
       "5281306                                      1.31  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Observe the data\n",
    "\n",
    "# look at just 1 dataset to get a feel for the data and check for missing values\n",
    "\n",
    "# 2018 quarter 3 weekdays\n",
    "# will need to download separately and store in working dir\n",
    "q3_2018_location = 'san_francisco-censustracts-2018-3-OnlyWeekdays-HourlyAggregate.csv'\n",
    "\n",
    "uber_df = pd.read_csv(q3_2018_location)\n",
    "uber_df.shape\n",
    "# set contains almost 10 million rows of data\n",
    "\n",
    "# check for missing values \n",
    "missing = uber_df.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing)\n",
    "\n",
    "# sort by hours of the day \n",
    "uber_df.sort_values('hod').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436433, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2: Filter the Data\n",
    "\n",
    "# filtered the data from files on my computer. I saved these files to a separate folder\n",
    "# so we would not need to keep running this time consuming data cleanup \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for root, dirs, files in os.walk(\"/Users/brendanwong/Desktop/DATACLEANUP\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            path = root +\"/\"+ file\n",
    "            df = pd.read_csv(path)\n",
    "            print(file + \" shape: \" + str(df.shape))\n",
    "            \n",
    "            df.sort_values('hod')\n",
    "            df['sourceid'].value_counts();\n",
    "            sources = df['sourceid'].value_counts() < 6000;\n",
    "            sources = sources.reset_index()\n",
    "            \n",
    "            for index, items in sources.iterrows():\n",
    "                if not items['sourceid']:\n",
    "                    sources.drop(index, inplace=True)\n",
    "            \n",
    "            print(\"sources shape: \" + str(sources.shape))\n",
    "            \n",
    "            for index, items in sources.iterrows():\n",
    "                df = df[df.sourceid != items['index']]\n",
    "                \n",
    "            print(file + \" shape: \" + str(df.shape))\n",
    "            \n",
    "            df['dstid'].value_counts()\n",
    "            destinations = df['dstid'].value_counts() < 3500\n",
    "            destinations = destinations.reset_index()\n",
    "            \n",
    "            for index, items in destinations.iterrows():\n",
    "                if not items['dstid']:\n",
    "                    destinations.drop(index, inplace=True)\n",
    "            \n",
    "            print(\"destinations shape: \" + str(destinations.shape))\n",
    "            \n",
    "            for index, items in destinations.iterrows():\n",
    "                df = df[df.dstid != items['index']]\n",
    "            \n",
    "            print(file + \" shape: \" + str(df.shape))\n",
    "            \n",
    "            hours = [0,1,2,3,4,5,6,11,12,13,14,20,21,22,23,24]\n",
    "            \n",
    "            for hour in hours:\n",
    "                df = df[df.hod != hour]\n",
    "                \n",
    "            print(file + \" shape: \" + str(df.shape))\n",
    "            \n",
    "            df.to_csv(\"filtered_\" + file)\n",
    "            print(\"FILTERED SHAPE: \" + str(df.shape))\n",
    "            print(\"\\n\\n\")\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"time:\\n\\n\\n\\n\")\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sourceid</th>\n",
       "      <th>dstid</th>\n",
       "      <th>hod</th>\n",
       "      <th>mean_travel_time</th>\n",
       "      <th>standard_deviation_travel_time</th>\n",
       "      <th>geometric_mean_travel_time</th>\n",
       "      <th>geometric_standard_deviation_travel_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>94.68</td>\n",
       "      <td>144.07</td>\n",
       "      <td>63.02</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "      <td>222.60</td>\n",
       "      <td>151.05</td>\n",
       "      <td>191.21</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>19</td>\n",
       "      <td>450.27</td>\n",
       "      <td>195.81</td>\n",
       "      <td>423.50</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>1424.28</td>\n",
       "      <td>366.28</td>\n",
       "      <td>1379.41</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>98</td>\n",
       "      <td>7</td>\n",
       "      <td>1191.91</td>\n",
       "      <td>302.11</td>\n",
       "      <td>1159.90</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sourceid  dstid  hod  mean_travel_time  \\\n",
       "0           7         9     20   10             94.68   \n",
       "1          27         9     79    8            222.60   \n",
       "2          28         9     80   19            450.27   \n",
       "3          29         9     81    9           1424.28   \n",
       "4          33         9     98    7           1191.91   \n",
       "\n",
       "   standard_deviation_travel_time  geometric_mean_travel_time  \\\n",
       "0                          144.07                       63.02   \n",
       "1                          151.05                      191.21   \n",
       "2                          195.81                      423.50   \n",
       "3                          366.28                     1379.41   \n",
       "4                          302.11                     1159.90   \n",
       "\n",
       "   geometric_standard_deviation_travel_time  \n",
       "0                                      2.48  \n",
       "1                                      1.71  \n",
       "2                                      1.39  \n",
       "3                                      1.29  \n",
       "4                                      1.25  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 3 a: Observe 1 filtered datafile and decide how to clean it up\n",
    "\n",
    "# read in filtered data into the dataframe for quarter 1, 2016\n",
    "df1_loc = 'filtered_san_francisco-censustracts-2016-1-OnlyWeekdays-HourlyAggregate.csv'\n",
    "df1 = pd.read_csv(df1_loc)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sourceid</th>\n",
       "      <th>dstid</th>\n",
       "      <th>mean_travel_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>94.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>222.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>450.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "      <td>1424.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>98</td>\n",
       "      <td>1191.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sourceid  dstid  mean_travel_time\n",
       "0         9     20             94.68\n",
       "1         9     79            222.60\n",
       "2         9     80            450.27\n",
       "3         9     81           1424.28\n",
       "4         9     98           1191.91"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 3 b: drop unnecessary columns \n",
    "df1.drop(columns=['Unnamed: 0', 'hod', 'standard_deviation_travel_time', 'geometric_mean_travel_time', 'geometric_standard_deviation_travel_time'], inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: method to find each route \n",
    "\n",
    "def find_route(df, year):\n",
    "    # route_name : mean_travel_time (across all hours)\n",
    "    route_data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        route_name = str(row['sourceid']) + '-' + str(row['dstid'])\n",
    "        if route_name not in route_data:\n",
    "            to_add = (row['mean_travel_time'], 1)\n",
    "            route_data[route_name] = to_add\n",
    "        else: \n",
    "            count = route_data[route_name][1] + 1\n",
    "            curr_sum = route_data[route_name][0] + row['mean_travel_time']\n",
    "            to_add = (curr_sum, count)\n",
    "            route_data[route_name] = to_add\n",
    "    # now find the mean time for each route \n",
    "    route_names = [] \n",
    "    route_times = [] \n",
    "    years = []\n",
    "    for route in route_data: \n",
    "        route_names.append(route)\n",
    "        route_info = route_data[route]\n",
    "        time = (route_info[0] / route_info[1])\n",
    "        route_times.append(time)\n",
    "        years.append(year)\n",
    "    new_df = pd.DataFrame(list(zip(route_names, route_times)), columns=['route_name', 'mean_travel_time'])\n",
    "    new_df['year'] = years\n",
    "    new_df.set_index('route_name')\n",
    "    return new_df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size\n",
      "1384290\n",
      "new size\n",
      "171615\n",
      "I eliminated this many rows:1212675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_name</th>\n",
       "      <th>mean_travel_time</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0-20.0</td>\n",
       "      <td>99.581111</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0-79.0</td>\n",
       "      <td>195.021111</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0-80.0</td>\n",
       "      <td>444.638889</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0-81.0</td>\n",
       "      <td>1473.006667</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0-98.0</td>\n",
       "      <td>1761.753333</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.0-113.0</td>\n",
       "      <td>725.247778</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0-155.0</td>\n",
       "      <td>3621.330000</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0-165.0</td>\n",
       "      <td>406.264444</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.0-80.0</td>\n",
       "      <td>497.136667</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0-81.0</td>\n",
       "      <td>1449.917778</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  route_name  mean_travel_time  year\n",
       "0   9.0-20.0         99.581111  2016\n",
       "1   9.0-79.0        195.021111  2016\n",
       "2   9.0-80.0        444.638889  2016\n",
       "3   9.0-81.0       1473.006667  2016\n",
       "4   9.0-98.0       1761.753333  2016\n",
       "5  9.0-113.0        725.247778  2016\n",
       "6  9.0-155.0       3621.330000  2016\n",
       "7  9.0-165.0        406.264444  2016\n",
       "8  20.0-80.0        497.136667  2016\n",
       "9  20.0-81.0       1449.917778  2016"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 5: Test find_route method \n",
    "\n",
    "# this is the size before condensing into routes \n",
    "print('original size')\n",
    "orig_size = df1.size\n",
    "print(df1.size)\n",
    "\n",
    "df1 = find_route(df1, 2016)\n",
    "\n",
    "# this is the size of the new dataframe with routes condensed \n",
    "print('new size')\n",
    "print(df1.size)\n",
    "new_size = df1.size\n",
    "rows_eliminated = orig_size - new_size\n",
    "\n",
    "print('I eliminated this many rows: ' + str(rows_eliminated))\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_san_francisco-censustracts-2016-4-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1258589, 8)\n",
      "new shape\n",
      "(163915, 3)\n",
      "I eliminated this many rows:4542611\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2017-3-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1525188, 8)\n",
      "new shape\n",
      "(199053, 3)\n",
      "I eliminated this many rows:5503593\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2018-2-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1950091, 8)\n",
      "new shape\n",
      "(256398, 3)\n",
      "I eliminated this many rows:7031170\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2017-4-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1740170, 8)\n",
      "new shape\n",
      "(228039, 3)\n",
      "I eliminated this many rows:6276563\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2018-1-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1847611, 8)\n",
      "new shape\n",
      "(240812, 3)\n",
      "I eliminated this many rows:6668008\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2016-3-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1154026, 8)\n",
      "new shape\n",
      "(149483, 3)\n",
      "I eliminated this many rows:4167655\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2017-2-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1342268, 8)\n",
      "new shape\n",
      "(174488, 3)\n",
      "I eliminated this many rows:4845608\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2018-3-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (2165382, 8)\n",
      "new shape\n",
      "(284889, 3)\n",
      "I eliminated this many rows:7806861\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2016-1-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (461430, 8)\n",
      "new shape\n",
      "(57205, 3)\n",
      "I eliminated this many rows:1674105\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2016-2-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (832066, 8)\n",
      "new shape\n",
      "(105596, 3)\n",
      "I eliminated this many rows:3011476\n",
      "\n",
      "\n",
      "\n",
      "filtered_san_francisco-censustracts-2017-1-OnlyWeekdays-HourlyAggregate.csv \n",
      "original shape: (1184196, 8)\n",
      "new shape\n",
      "(152736, 3)\n",
      "I eliminated this many rows:4278576\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Run find_route method on all filtered dataset files \n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"/Users/zoeychesny/Desktop/filtered_data\"):\n",
    "    cleaned_dfs = [] \n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            path = root + '/' + file\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            print(file + \" \\noriginal shape: \" + str(df.shape))\n",
    "\n",
    "            #drop unnecessary columns \n",
    "            df.drop(columns=['hod', 'standard_deviation_travel_time', 'geometric_mean_travel_time', 'geometric_standard_deviation_travel_time'], inplace=True)\n",
    "            \n",
    "            \n",
    "            # this is the size before condensing into routes \n",
    "            orig_size = df.size\n",
    "            year = int(file[36:40])\n",
    "            quarter = int(file[41])\n",
    "\n",
    "            df = find_route(df, 2016)\n",
    "\n",
    "            # this is the size of the new dataframe with routes condensed \n",
    "            print('new shape')\n",
    "            print(df.shape)\n",
    "            new_size = df.size\n",
    "            rows_eliminated = orig_size - new_size\n",
    "\n",
    "            print('I eliminated this many rows:' + str(rows_eliminated))\n",
    "            \n",
    "            df.to_csv(\"df_q\" + str(quarter) + \"_y\"+ str(year) + \".csv\")\n",
    "\n",
    "            cleaned_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: create route_matching method \n",
    "find routes that are the same across all 11 quarters \n",
    "\n",
    "def route_matching(list_of_dfs):\n",
    "    base = list_of_dfs[0]\n",
    "    q1 = [0, 4, 8]\n",
    "    q2 = [1, 5, 9]\n",
    "    q3 = [2, 6, 10]\n",
    "    q4 = [3, 7]\n",
    "    all_routes = [] \n",
    "    q1_time = []\n",
    "    q2_time = [] \n",
    "    q3_time = [] \n",
    "    q4_time = [] \n",
    "    \n",
    "    quarters = [q1, q2, q3, q4]\n",
    "    q_time = [q1_time, q2_time, q3_time, q4_time]\n",
    "    \n",
    "    route_count = 0 \n",
    "    \n",
    "    for index, row in base.iterrows():\n",
    "        r_name = row['route_name']\n",
    "           # now check all the other dfs for this route\n",
    "        if is_route_in_all(r_name, list_of_dfs) == True:\n",
    "            # find 100 routes across the datasets \n",
    "            if route_count > 100:\n",
    "                break\n",
    "            all_routes.append(r_name)\n",
    "            route_count = route_count + 1\n",
    "            q_num = 0 \n",
    "\n",
    "                # iterate for each quarter \n",
    "            for i in range(4):\n",
    "                time_sum = 0 \n",
    "                for j in quarters[i]:\n",
    "                    time_sum = time_sum + extract_route_time(list_of_dfs[j], r_name)\n",
    "                time_mean = time_sum / len(quarters[i])\n",
    "                q_time[i].append(time_mean)\n",
    "\n",
    "        new_df = pd.DataFrame(columns=['route_name', 'Q1_mean_travel_time', 'Q2_mean_travel_time', 'Q3_mean_travel_time', 'Q4_mean_travel_time'])\n",
    "        new_df['route_name'] = all_routes\n",
    "        new_df['Q1_mean_travel_time'] = q1_time\n",
    "        new_df['Q2_mean_travel_time'] = q2_time\n",
    "        new_df['Q3_mean_travel_time'] = q3_time\n",
    "        new_df['Q4_mean_travel_time'] = q4_time\n",
    "    return new_df\n",
    "    \n",
    "\n",
    "def is_route_in_all(route, list_of_dfs):\n",
    "    for df in list_of_dfs: \n",
    "        check = list(df['route_name'])\n",
    "        if route not in check:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# route_time\n",
    "def extract_route_time(df, route_name):\n",
    "    row = df.loc[df['route_name'] == route_name]\n",
    "    time_list = list(row['mean_travel_time'])\n",
    "    return float(time_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is how the files are read so the above cleaning steps do not need to be run mutliple times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new dataframes from filtered data \n",
    "\n",
    "new_dfs = [] \n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"/Users/zoeychesny/Desktop/COGS-108-Final-Project\"):\n",
    "    cleaned_dfs = [] \n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file[0:2] == 'df':\n",
    "                path = root +\"/\"+ file\n",
    "                df = pd.read_csv(path)\n",
    "                df.drop(columns='Unnamed: 0', inplace=True)\n",
    "                new_dfs.append(df)\n",
    "                \n",
    "is_route_in_all('9.0-20.0', new_dfs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_name</th>\n",
       "      <th>Q1_mean_travel_time</th>\n",
       "      <th>Q2_mean_travel_time</th>\n",
       "      <th>Q3_mean_travel_time</th>\n",
       "      <th>Q4_mean_travel_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2363.0-2525.0</td>\n",
       "      <td>1229.138519</td>\n",
       "      <td>1221.216296</td>\n",
       "      <td>1259.281481</td>\n",
       "      <td>1252.651111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315.0-433.0</td>\n",
       "      <td>995.164444</td>\n",
       "      <td>1063.062963</td>\n",
       "      <td>1103.779630</td>\n",
       "      <td>1019.523333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>314.0-443.0</td>\n",
       "      <td>1602.477407</td>\n",
       "      <td>1568.980370</td>\n",
       "      <td>1588.975926</td>\n",
       "      <td>1618.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1733.0-1361.0</td>\n",
       "      <td>651.382593</td>\n",
       "      <td>736.626667</td>\n",
       "      <td>721.991481</td>\n",
       "      <td>667.976667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743.0-1261.0</td>\n",
       "      <td>2392.205185</td>\n",
       "      <td>2555.788519</td>\n",
       "      <td>2500.797037</td>\n",
       "      <td>2447.785556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      route_name  Q1_mean_travel_time  Q2_mean_travel_time  \\\n",
       "0  2363.0-2525.0          1229.138519          1221.216296   \n",
       "1    315.0-433.0           995.164444          1063.062963   \n",
       "2    314.0-443.0          1602.477407          1568.980370   \n",
       "3  1733.0-1361.0           651.382593           736.626667   \n",
       "4  1743.0-1261.0          2392.205185          2555.788519   \n",
       "\n",
       "   Q3_mean_travel_time  Q4_mean_travel_time  \n",
       "0          1259.281481          1252.651111  \n",
       "1          1103.779630          1019.523333  \n",
       "2          1588.975926          1618.810000  \n",
       "3           721.991481           667.976667  \n",
       "4          2500.797037          2447.785556  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allquarter_df = route_matching(new_dfs)\n",
    "print(allquarter_df.shape)\n",
    "allquarter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 5)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allquarter_df.to_csv('allquarter.csv')\n",
    "allquarter_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files \n",
    "new_dfs = [] \n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"/Users/zoeychesny/Desktop/COGS-108-Final-Project\"):\n",
    "    cleaned_dfs = [] \n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file[0:2] == 'df':\n",
    "                path = root +\"/\"+ file\n",
    "                df = pd.read_csv(path)\n",
    "                df.drop(columns='Unnamed: 0', inplace=True)\n",
    "                new_dfs.append(df)\n",
    "                \n",
    "allquarter_df = pd.read_csv('allquarter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include cells that describe the steps in your data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that changes the quarter names to numeric values from 1-11, 1 being the earliest quarter and 11 being the latest, so they can be used for multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_quarter(quarter):\n",
    "\n",
    "    quarter = quarter.lower()\n",
    "    \n",
    "    quarter = quarter.strip()\n",
    "    \n",
    "    quarter = quarter.replace('q', '')\n",
    "    quarter = quarter.replace('116', '1')\n",
    "    quarter = quarter.replace('216', '2')\n",
    "    quarter = quarter.replace('316', '3')\n",
    "    quarter = quarter.replace('416', '4')\n",
    "    quarter = quarter.replace('117', '5')\n",
    "    quarter = quarter.replace('217', '6')\n",
    "    quarter = quarter.replace('317', '7')\n",
    "    quarter = quarter.replace('417', '8')\n",
    "    quarter = quarter.replace('118', '9')\n",
    "    quarter = quarter.replace('218', '10')\n",
    "    quarter = quarter.replace('318', '11')\n",
    "    \n",
    "    quarter = quarter.strip()\n",
    "    \n",
    "    return int(quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function to data for all 4 routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_route in dfs_route_1:\n",
    "    df_route['quarter'] = df_route['quarter'].apply(standardize_quarter)\n",
    "for df_route in dfs_route_2:\n",
    "    df_route['quarter'] = df_route['quarter'].apply(standardize_quarter)\n",
    "for df_route in dfs_route_3:\n",
    "    df_route['quarter'] = df_route['quarter'].apply(standardize_quarter)\n",
    "for df_route in dfs_route_4:\n",
    "    df_route['quarter'] = df_route['quarter'].apply(standardize_quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use linear models to check if there is a significant difference in mean drive time during each quarter over each route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for df_route in dfs_route_1:\n",
    "    outcome_route1, predictors_route1 = patsy.dmatrices('quarter ~ mean_travel_time', df_route)\n",
    "    mod_route1 = sm.OLS(outcome_route1, predictors_route1)\n",
    "    res_route1 = mod_route1.fit()\n",
    "\n",
    "    p_value1 = res_route1.pvalues[1]\n",
    "    \n",
    "    if p_value1 < 0.01:\n",
    "        print('There is a significant difference in mean drive time over route 1 during quarter', df_route['quarter'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_route in dfs_route_2:\n",
    "    outcome_route2, predictors_route2 = patsy.dmatrices('quarter ~ mean_travel_time', df_route)\n",
    "    mod_route2 = sm.OLS(outcome_route2, predictors_route2)\n",
    "    res_route2 = mod_route2.fit()\n",
    "\n",
    "    p_value2 = res_route2.pvalues[1]\n",
    "    \n",
    "    if p_value2 < 0.01:\n",
    "        print('There is a significant difference in mean drive time over route 2 during quarter', df_route['quarter'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_route in dfs_route_3:\n",
    "    outcome_route3, predictors_route3 = patsy.dmatrices('quarter ~ mean_travel_time', df_route)\n",
    "    mod_route3 = sm.OLS(outcome_route3, predictors_route3)\n",
    "    res_route3 = mod_route3.fit()\n",
    "\n",
    "    p_value3 = res_route3.pvalues[1]\n",
    "    \n",
    "    if p_value3 < 0.01:\n",
    "        print('There is a significant difference in mean drive time over route 3 during quarter', df_route['quarter'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_route in dfs_route_4:\n",
    "    outcome_route4, predictors_route4 = patsy.dmatrices('quarter ~ mean_travel_time', df_route)\n",
    "    mod_route4 = sm.OLS(outcome_route4, predictors_route4)\n",
    "    res_route4 = mod_route4.fit()\n",
    "\n",
    "    p_value4 = res_route4.pvalues[1]\n",
    "    \n",
    "    if p_value4 < 0.01:\n",
    "        print('There is a significant difference in mean drive time over route 4 during quarter', df_route['quarter'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy\n",
    "\n",
    "Privacy can be guaranteed from both Uber and our analysis. User data from the dataset itself is anonymized and aggregated to ensure no personally identifiable information or user behavior can be surfaced. The dataset  only contains the average time from source to destination with no individual user data. The safe harbor method was already implemented in the data we were given, so we did not have to take further measures to anonymize the data. The geographic locations were encoded in numbered indices to ensure the specific geographical location of each route was kept private. There were no unique identifiers in the given dataset. \n",
    "\n",
    "## Ethics \n",
    "\n",
    "The goal of our project was not to make any general claims about how traffic is influenced by the season, but rather to apply methods and concepts we learned in class to real-life data as a data science investigation. We do not claim to know the travel time and traffic across the world or even in the US, but instead specifically analyzed the Uber travel patterns in San Francisco across the changing seasons from 2016-2018. The results of our data are not inherently harmful to anyone, but they could potentially impact the way Uber runs their company. While riders may want to get to their destinations as fast as possible, it still needs to be recognized that Uber cannot control traffic. No one wants increased travel times, so this could cause Uber to look into ways to improve their service especially in congested areas during peak commute times. However, it is important to realize that the results of our analysis are not intended to suggest anything about Uber as a company, but simply to analyze how the Uber travel times changes over the quarters. Our data is biased because it only accounts for Uber users, but this is acceptable in our analysis since we do not use our conclusions to make general claims about all people commuting in the San Francisco area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your discussion information here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
